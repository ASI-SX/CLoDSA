{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/joheras/CLoDSA/blob/master/notebooks/CLODSA_Video_Classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLTwDWw0ni8G"
   },
   "source": [
    "# Augmenting a dataset for action recognition in videos\n",
    "\n",
    "In this notebook, we illustrate how CLODSA can be employed to augment a dataset of videos devoted to action recognition. In particular, we use the [UCF 101 dataset](http://crcv.ucf.edu/data/UCF101.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPDdsBtFni8I"
   },
   "source": [
    "The UCF 101 dataset contains 13320 videos from 101 action categories. In our case, we will use a subset of this dataset to illustrate the use of CLoDSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4024
    },
    "colab_type": "code",
    "id": "FbGwos-bni8K",
    "outputId": "3151302c-730f-4c18-d751-3dd36c4ed51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-18 18:39:31--  https://www.dropbox.com/s/ik98mw8qbuekv31/UCF-101-subset.zip?dl=0\n",
      "Resolviendo www.dropbox.com (www.dropbox.com)... 162.125.68.1, 2620:100:6024:1::a27d:4401\n",
      "Conectando con www.dropbox.com (www.dropbox.com)[162.125.68.1]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 301 Moved Permanently\n",
      "Ubicación: /s/raw/ik98mw8qbuekv31/UCF-101-subset.zip [siguiente]\n",
      "--2019-01-18 18:39:31--  https://www.dropbox.com/s/raw/ik98mw8qbuekv31/UCF-101-subset.zip\n",
      "Reutilizando la conexión con www.dropbox.com:443.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com/cd/0/inline/AZo-lk6R2UYzi8cUKj97Me436-2oO7O1HgwJelqN4fDSDSMAs6km9MP-IXzi0y7aURxZsHOVNvNOTUqZr0mc1CVTkGcmfLXv5TRz_eZRDUKm29Dpe89M-4dIdcNaN6olvDXZTf3gWdig-1UoGCos9wq1sXDqeZT0JwQOitDRaKJmVr9T24WZe7wJrGyKlN-Ce4U/file# [siguiente]\n",
      "--2019-01-18 18:39:32--  https://ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com/cd/0/inline/AZo-lk6R2UYzi8cUKj97Me436-2oO7O1HgwJelqN4fDSDSMAs6km9MP-IXzi0y7aURxZsHOVNvNOTUqZr0mc1CVTkGcmfLXv5TRz_eZRDUKm29Dpe89M-4dIdcNaN6olvDXZTf3gWdig-1UoGCos9wq1sXDqeZT0JwQOitDRaKJmVr9T24WZe7wJrGyKlN-Ce4U/file\n",
      "Resolviendo ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com (ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com)... 162.125.68.6, 2620:100:6024:6::a27d:4406\n",
      "Conectando con ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com (ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com)[162.125.68.6]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 FOUND\n",
      "Ubicación: /cd/0/inline2/AZoYT2-Ju4g89rAsv6ykzgdrGOILgfs0mZbuWfwArfjG4f2iHmBHN12HkZaEgOBZGiYE7bFVNZ7pksaj0KI-ipOC7n9q6BA_0yWOnKC63EFgE9gJYIY8pqQTofR5Le05BZ6kmiUsNw1vDjWAmdnitJPnMwFsSNfgKteG_AxYMMcuGzgoFbVkus62ju2tEchcOZwRMT7kzKiKVhsm92AVyjfDB1NZion54J8EA-DSnZ35cFEZbWbkLfP4Vd5pvnz3WUwt-6x4qQi6nizF0ikXcX82B8uaH8zMGfNDBn0o2dzxAy7EpuaQyo-JbZxYDNcF-76r0YVUj7qiKSphrYj1EBRBtczJX1gG_vfKg2_gfiX2urG-_4BPk9QN0WFWpM3rXVFAMiVuJURFsOGOPhYcADrufOubVH8KoX8a98pjgeFN-Nk1ftbW8-j3haAvAJuu0TM/file [siguiente]\n",
      "--2019-01-18 18:39:33--  https://ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com/cd/0/inline2/AZoYT2-Ju4g89rAsv6ykzgdrGOILgfs0mZbuWfwArfjG4f2iHmBHN12HkZaEgOBZGiYE7bFVNZ7pksaj0KI-ipOC7n9q6BA_0yWOnKC63EFgE9gJYIY8pqQTofR5Le05BZ6kmiUsNw1vDjWAmdnitJPnMwFsSNfgKteG_AxYMMcuGzgoFbVkus62ju2tEchcOZwRMT7kzKiKVhsm92AVyjfDB1NZion54J8EA-DSnZ35cFEZbWbkLfP4Vd5pvnz3WUwt-6x4qQi6nizF0ikXcX82B8uaH8zMGfNDBn0o2dzxAy7EpuaQyo-JbZxYDNcF-76r0YVUj7qiKSphrYj1EBRBtczJX1gG_vfKg2_gfiX2urG-_4BPk9QN0WFWpM3rXVFAMiVuJURFsOGOPhYcADrufOubVH8KoX8a98pjgeFN-Nk1ftbW8-j3haAvAJuu0TM/file\n",
      "Reutilizando la conexión con ucfb9e0bef0835df43275ff882ec.dl.dropboxusercontent.com:443.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 9747960 (9,3M) [application/zip]\n",
      "Guardando como: “UCF-101-subset.zip”\n",
      "\n",
      "UCF-101-subset.zip  100%[===================>]   9,30M  6,41MB/s    en 1,5s    \n",
      "\n",
      "2019-01-18 18:39:35 (6,41 MB/s) - “UCF-101-subset.zip” guardado [9747960/9747960]\n",
      "\n",
      "Archive:  UCF-101-subset.zip\n",
      "   creating: UCF-101-subset/\n",
      "   creating: UCF-101-subset/ApplyEyeMakeup/\n",
      "  inflating: UCF-101-subset/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi  \n",
      "  inflating: UCF-101-subset/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c03.avi  \n",
      "  inflating: UCF-101-subset/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c02.avi  \n",
      "  inflating: UCF-101-subset/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi  \n",
      "  inflating: UCF-101-subset/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c05.avi  \n",
      "   creating: UCF-101-subset/Biking/\n",
      "  inflating: UCF-101-subset/Biking/v_Biking_g01_c04.avi  \n",
      "  inflating: UCF-101-subset/Biking/v_Biking_g05_c03.avi  \n",
      "  inflating: UCF-101-subset/Biking/v_Biking_g05_c06.avi  \n",
      "  inflating: UCF-101-subset/Biking/v_Biking_g06_c02.avi  \n",
      "  inflating: UCF-101-subset/Biking/v_Biking_g08_c05.avi  \n",
      "   creating: UCF-101-subset/Diving/\n",
      "  inflating: UCF-101-subset/Diving/v_Diving_g01_c01.avi  \n",
      "  inflating: UCF-101-subset/Diving/v_Diving_g01_c06.avi  \n",
      "  inflating: UCF-101-subset/Diving/v_Diving_g04_c07.avi  \n",
      "  inflating: UCF-101-subset/Diving/v_Diving_g05_c01.avi  \n",
      "  inflating: UCF-101-subset/Diving/v_Diving_g06_c05.avi  \n",
      "   creating: UCF-101-subset/Fencing/\n",
      "  inflating: UCF-101-subset/Fencing/v_Fencing_g02_c05.avi  \n",
      "  inflating: UCF-101-subset/Fencing/v_Fencing_g05_c02.avi  \n",
      "  inflating: UCF-101-subset/Fencing/v_Fencing_g09_c03.avi  \n",
      "  inflating: UCF-101-subset/Fencing/v_Fencing_g12_c05.avi  \n",
      "  inflating: UCF-101-subset/Fencing/v_Fencing_g15_c02.avi  \n",
      "   creating: UCF-101-subset/Haircut/\n",
      "  inflating: UCF-101-subset/Haircut/v_Haircut_g01_c04.avi  \n",
      "  inflating: UCF-101-subset/Haircut/v_Haircut_g03_c06.avi  \n",
      "  inflating: UCF-101-subset/Haircut/v_Haircut_g05_c03.avi  \n",
      "  inflating: UCF-101-subset/Haircut/v_Haircut_g07_c06.avi  \n",
      "  inflating: UCF-101-subset/Haircut/v_Haircut_g10_c03.avi  \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/ik98mw8qbuekv31/UCF-101-subset.zip?dl=0 -O UCF-101-subset.zip\n",
    "!unzip UCF-101-subset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmaoZcYXni8Y"
   },
   "source": [
    "We can check the amount of videos in each one of the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "yXoVkFkqni8a",
    "outputId": "55518f50-f4a3-4273-e5a5-dec4c2bd51fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders\n",
      "ApplyEyeMakeup\tBiking\tDiving\tFencing  Haircut\n",
      "Number of videos in ApplyEyeMakeUp folder\n",
      "5\n",
      "Number of videos in Biking\n",
      "5\n",
      "Number of videos in Diving\n",
      "5\n",
      "Number of videos in Fencing\n",
      "5\n",
      "Number of videos in Haircut\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(\"Folders\")\n",
    "!ls UCF-101-subset/\n",
    "print(\"Number of videos in ApplyEyeMakeUp folder\")\n",
    "!ls UCF-101-subset/ApplyEyeMakeup/ | wc -l\n",
    "print(\"Number of videos in Biking\")\n",
    "!ls UCF-101-subset/Biking/ | wc -l\n",
    "print(\"Number of videos in Diving\")\n",
    "!ls UCF-101-subset/Diving/ | wc -l\n",
    "print(\"Number of videos in Fencing\")\n",
    "!ls UCF-101-subset/Fencing/ | wc -l\n",
    "print(\"Number of videos in Haircut\")\n",
    "!ls UCF-101-subset/Haircut/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMaT8vnzni8i"
   },
   "source": [
    "## Augmentation techniques\n",
    "\n",
    "In this notebook, we illustrate how the following augmentation techniques can be applied to the downloaded dataset:\n",
    "- Horizontal flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwG_kg6Tni8k"
   },
   "source": [
    "## Installing the necessary libraries\n",
    "\n",
    "In case that CLODSA is not installed in your system, the first task consists in installing it using ``pip``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "id": "z7qMC41Xni8k",
    "outputId": "51dca5c0-72ec-4fe2-cdce-81a4fb5c5873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clodsa in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (1.2.19)\r\n",
      "Requirement already satisfied: imutils in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (0.5.1)\r\n",
      "Requirement already satisfied: scikit-learn in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (0.19.2)\r\n",
      "Requirement already satisfied: numpy in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (1.15.4)\r\n",
      "Requirement already satisfied: mahotas in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (1.4.5)\r\n",
      "Requirement already satisfied: scipy in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (1.1.0)\r\n",
      "Requirement already satisfied: h5py in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (2.8.0)\r\n",
      "Requirement already satisfied: progressbar2 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (3.38.0)\r\n",
      "Requirement already satisfied: Keras in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (2.2.4)\r\n",
      "Requirement already satisfied: commentjson in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa) (0.7.1)\r\n",
      "Requirement already satisfied: six in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from h5py->clodsa) (1.11.0)\r\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from progressbar2->clodsa) (2.3.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from Keras->clodsa) (3.13)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from Keras->clodsa) (1.0.5)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from Keras->clodsa) (1.0.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install clodsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoxKQxZJni8q"
   },
   "source": [
    "## Loading the necessary libraries\n",
    "\n",
    "The first step in the pipeline consists in loading the necessary libraries to apply the data augmentation techniques in CLODSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yzMR_9kBni8s",
    "outputId": "16fe5d55-95d6-4329-ad41-a55aa19afdb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from clodsa.augmentors.augmentorFactory import createAugmentor\n",
    "from clodsa.transformers.transformerFactory import transformerGenerator\n",
    "from clodsa.techniques.techniqueFactory import createTechnique\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85NJIbHRni8y"
   },
   "source": [
    "## Creating the augmentor object\n",
    "\n",
    "As explained in the documentation of CLODSA, we need to specify some parameters for the augmentation process, and use them to create an augmentor object.  \n",
    "\n",
    "_The kind of problem_. In this case, we are working in a videoclassification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udjKKpIMni80"
   },
   "outputs": [],
   "source": [
    "PROBLEM = \"stackclassification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYs7FXmFni86"
   },
   "source": [
    "_The annotation mode_. The annotation is provided by the name of the folder containing the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROmfoBePni88"
   },
   "outputs": [],
   "source": [
    "ANNOTATION_MODE = \"videofolders\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFmLaCVzni9A"
   },
   "source": [
    "_The input path_. The input path containing the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4qLXr7Hni9E"
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = \"UCF-101-subset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsxyfUVVni9K"
   },
   "source": [
    "_The generation mode_. In this case, linear, that is, all the augmentation techniques are applied to all the videos of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXHMv0Djni9K"
   },
   "outputs": [],
   "source": [
    "GENERATION_MODE = \"linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P03YdvTTni9Q"
   },
   "source": [
    "_The output mode_. The generated videos will be stored in a new folder called augmented_videos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFboWqIcni9Q"
   },
   "outputs": [],
   "source": [
    "OUTPUT_MODE = \"videofolders\"\n",
    "OUTPUT_PATH= \"augmented_videos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG1harcAni9U"
   },
   "source": [
    "Using the above information, we can create our augmentor object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OH7Xy3rIni9W"
   },
   "outputs": [],
   "source": [
    "augmentor = createAugmentor(PROBLEM,ANNOTATION_MODE,OUTPUT_MODE,GENERATION_MODE,INPUT_PATH,{\"outputPath\":OUTPUT_PATH})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3es15zWQni9a"
   },
   "source": [
    "## Adding the augmentation techniques\n",
    "\n",
    "Now, we define the techniques that will be applied in our augmentation process and add them to our augmentor object. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we must define a transformer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformerGenerator(PROBLEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zpB7VSoni9-"
   },
   "source": [
    "#### Horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6GhHPJxni-A"
   },
   "outputs": [],
   "source": [
    "hFlip = createTechnique(\"flip\",{\"flip\":1})\n",
    "augmentor.addTransformer(transformer(hFlip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTZLwyr_ni-4"
   },
   "source": [
    "#### None\n",
    "(to keep also the original video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XnKlw1Vni-4"
   },
   "outputs": [],
   "source": [
    "none = createTechnique(\"none\",{})\n",
    "augmentor.addTransformer(transformer(none))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nInbOZQHni_A"
   },
   "source": [
    "## Applying the augmentation process\n",
    "\n",
    "Finally, we apply the augmentation process (this might take some time depending on the number of videos of the original dataset and the number of transformations that will be applied). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVuonlNLni_A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    }
   ],
   "source": [
    "augmentor.applyAugmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdW54yfhni_C"
   },
   "source": [
    "Finally, we can check the amount of videos in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yR5dWTCDni_E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos in ApplyEyeMakeUp folder\n",
      "10\n",
      "Number of videos in Biking\n",
      "10\n",
      "Number of videos in Diving\n",
      "10\n",
      "Number of videos in Fencing\n",
      "10\n",
      "Number of videos in Haircut\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of videos in ApplyEyeMakeUp folder\")\n",
    "!ls augmented_videos/ApplyEyeMakeup/ | wc -l\n",
    "print(\"Number of videos in Biking\")\n",
    "!ls augmented_videos/Biking/ | wc -l\n",
    "print(\"Number of videos in Diving\")\n",
    "!ls augmented_videos/Diving/ | wc -l\n",
    "print(\"Number of videos in Fencing\")\n",
    "!ls augmented_videos/Fencing/ | wc -l\n",
    "print(\"Number of videos in Haircut\")\n",
    "!ls augmented_videos/Haircut/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jf9ab-kZa1Rw"
   },
   "source": [
    "If you are executing this notebook in Colaboratory, you need to download the generated files. To that aim, you can create a zip folder and download it using the following commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HT3J6-Sra_jg"
   },
   "outputs": [],
   "source": [
    "!zip -r augmented_videos.zip augmented_videos\n",
    "from google.colab import files\n",
    "files.download('augmented_videos.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CLODSA_Melanoma.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
