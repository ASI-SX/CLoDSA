{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/joheras/CLoDSA/blob/master/notebooks/CLODSA_YOLO.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EzlBCSXafQp"
   },
   "source": [
    "# Augmenting a dataset for object detection in YOLO\n",
    "\n",
    "In this notebook, we illustrate how CLODSA can be employed to augment a dataset of images devoted to object detection using the [YOLO format](https://pjreddie.com/darknet/yolo/). In particular, we use a subset of the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/). We will use 20 images from that dataset. Such a subset can be downloaded by executing the following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1341
    },
    "colab_type": "code",
    "id": "CUN3bp7kafQt",
    "outputId": "345a2ae5-2936-4eb2-fe75-5f4fd33c15d7"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/7j00clf7tmn1ilm/PascalVOCYOLO.zip?dl=0 -O PascalVOCYOLO.zip\n",
    "!unzip PascalVOCYOLO.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhjRCzmKafQ4"
   },
   "source": [
    "We can check the amount of images in each one of the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "He0Hu0_BafQ6",
    "outputId": "db625f11-746b-47f3-f7dc-595014324bc6"
   },
   "outputs": [],
   "source": [
    "print(\"Number of images in the folder\")\n",
    "!ls -1 PascalVOCYOLO/*.jpg | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5n9M6O7oafRP"
   },
   "source": [
    "## Augmentation techniques\n",
    "\n",
    "In this notebook, we will use the following augmentation techniques:\n",
    "- Vertical, horizontal, and vertical-horizontal flips.\n",
    "- 180ยบ Rotation.\n",
    "- Average blurring.\n",
    "- Raise the hue value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUeICW7VafRQ"
   },
   "source": [
    "## Installing the necessary libraries\n",
    "\n",
    "In case that CLODSA is not installed in your system, the first task consists in installing it using ``pip``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "id": "7IMnZaD7afRS",
    "outputId": "c061b425-fd04-4d99-bb32-fa7b47857068"
   },
   "outputs": [],
   "source": [
    "!pip install clodsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxA9V3xGafRW"
   },
   "source": [
    "## Loading the necessary libraries\n",
    "\n",
    "The first step in the pipeline consists in loading the necessary libraries to apply the data augmentation techniques in CLODSA. We also load some libraries to show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qO1fejKIafRZ",
    "outputId": "10838228-0a16-4768-8596-59296d797ab8"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from clodsa.augmentors.augmentorFactory import createAugmentor\n",
    "from clodsa.transformers.transformerFactory import transformerGenerator\n",
    "from clodsa.techniques.techniqueFactory import createTechnique\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_0autm-afRe"
   },
   "source": [
    "## Creating the augmentor object\n",
    "\n",
    "As explained in the documentation of CLODSA, we need to specify some parameters for the augmentation process, and use them to create an augmentor object.  \n",
    "\n",
    "_The kind of problem_. In this case, we are working in a detection problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxz6HsATafRg"
   },
   "outputs": [],
   "source": [
    "PROBLEM = \"detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6O4tVUPafRl"
   },
   "source": [
    "_The annotation mode_. We use the YOLO format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNHRztstafRm"
   },
   "outputs": [],
   "source": [
    "ANNOTATION_MODE = \"yolo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SiKYMZBYafRq"
   },
   "source": [
    "_The input path_. The input path containing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pzsCqaXafRs"
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = \"PascalVOCYOLO/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XM6Kzl-OafRw"
   },
   "source": [
    "_The generation mode_. In this case, linear, that is, all the augmentation techniques are applied to all the images of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Tq04z7nafRx"
   },
   "outputs": [],
   "source": [
    "GENERATION_MODE = \"linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQLgW6izafR1"
   },
   "source": [
    "_The output mode_. The generated images will be stored in a new folder called augmented_images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUWyaD8rafR2"
   },
   "outputs": [],
   "source": [
    "OUTPUT_MODE = \"yolo\"\n",
    "OUTPUT_PATH= \"augmented_images_yolo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwV-nrdJafR5"
   },
   "source": [
    "Using the above information, we can create our augmentor object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTaCIQD-afR7"
   },
   "outputs": [],
   "source": [
    "augmentor = createAugmentor(PROBLEM,ANNOTATION_MODE,OUTPUT_MODE,GENERATION_MODE,INPUT_PATH,{\"outputPath\":OUTPUT_PATH})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxoQQ3wFafSA"
   },
   "source": [
    "## Adding the augmentation techniques\n",
    "\n",
    "Now, we define the techniques that will be applied in our augmentation process and add them to our augmentor object. To illustrate the transformations, we will use the following image of the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "oaQlDpj2afSB",
    "outputId": "fa539f8b-6ab2-4ae6-d99e-00126c7061fc"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"PascalVOCYOLO/000001.jpg\")\n",
    "# changing to the BGR format of OpenCV to RGB format for matplotlib\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVnUVKZIafSH"
   },
   "source": [
    "Just for showing the results of applying data augmentation in an object detection problem, we define a function to read the annotations and another one to show them. This funcionality is not necessary when using CLODSA since it is already implemented in there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWzxSnYPafSJ"
   },
   "outputs": [],
   "source": [
    "def boxesFromYOLO(imagePath,labelPath):\n",
    "    image = cv2.imread(imagePath)\n",
    "    (hI, wI) = image.shape[:2]\n",
    "    lines = [line.rstrip('\\n') for line in open(labelPath)]\n",
    "    #if(len(objects)<1):\n",
    "    #    raise Exception(\"The xml should contain at least one object\")\n",
    "    boxes = []\n",
    "    if lines != ['']:\n",
    "        for line in lines:\n",
    "            components = line.split(\" \")\n",
    "            category = components[0]\n",
    "            x  = int(float(components[1])*wI - float(components[3])*wI/2)\n",
    "            y = int(float(components[2])*hI - float(components[4])*hI/2)\n",
    "            h = int(float(components[4])*hI)\n",
    "            w = int(float(components[3])*wI)\n",
    "            boxes.append((category, (x, y, w, h)))\n",
    "    return (image,boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4_Jmo5NafSM"
   },
   "outputs": [],
   "source": [
    "categoriesColors = {11: (255,0,0),14:(0,0,255)}\n",
    "\n",
    "def showBoxes(image,boxes):\n",
    "    cloneImg = image.copy()\n",
    "    for box in boxes:\n",
    "        if(len(box)==2):\n",
    "            (category, (x, y, w, h))=box\n",
    "        else:\n",
    "            (category, (x, y, w, h),_)=box\n",
    "        if int(category) in categoriesColors.keys():\n",
    "            cv2.rectangle(cloneImg,(x,y),(x+w,y+h),categoriesColors[int(category)],5)\n",
    "        else:\n",
    "            cv2.rectangle(cloneImg,(x,y),(x+w,y+h),(0,255,0),5)\n",
    "    plt.imshow(cloneImg[:,:,::-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUc0j53dafSO"
   },
   "source": [
    "Now, we show the annotation of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "SyopvrsPafSQ",
    "outputId": "c16ebf65-02d1-434b-f6ea-9def904bb6b4"
   },
   "outputs": [],
   "source": [
    "img,boxes = boxesFromYOLO(\"PascalVOCYOLO/000001.jpg\",\"PascalVOCYOLO/000001.txt\")\n",
    "showBoxes(img,boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we must define a transformer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformerGenerator(PROBLEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vax19fU3afSV"
   },
   "source": [
    "#### Vertical flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60yK0EbHafSW"
   },
   "outputs": [],
   "source": [
    "vFlip = createTechnique(\"flip\",{\"flip\":0})\n",
    "augmentor.addTransformer(transformer(vFlip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFAtSKmTafSY"
   },
   "source": [
    "Showing the result of applying the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "colab_type": "code",
    "id": "sSTjCJQ4afSZ",
    "outputId": "e5fc8cca-6263-4131-c57c-90380654dac8"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "showBoxes(img,boxes)\n",
    "vFlipGenerator = transformer(vFlip)\n",
    "vFlipImg,vFlipBoxes = vFlipGenerator.transform(img,boxes)\n",
    "plt.figure()\n",
    "plt.title(\"Transformed\")\n",
    "showBoxes(vFlipImg,vFlipBoxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3fsGvQfafSc"
   },
   "source": [
    "#### Horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xx7VxTUSafSe"
   },
   "outputs": [],
   "source": [
    "hFlip = createTechnique(\"flip\",{\"flip\":1})\n",
    "augmentor.addTransformer(transformer(hFlip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMDc9LTqafSh"
   },
   "source": [
    "Showing the result of applying the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQadcJ1SafSi",
    "outputId": "bbc7023d-c16d-4961-c49b-1a5df063951b"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "showBoxes(img,boxes)\n",
    "hFlipGenerator = transformer(hFlip)\n",
    "hFlipImg,hFlipBoxes = hFlipGenerator.transform(img,boxes)\n",
    "plt.figure()\n",
    "plt.title(\"Transformed\")\n",
    "showBoxes(hFlipImg,hFlipBoxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWGlYPUjafSm"
   },
   "source": [
    "#### Horizontal and vertical flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-A_1OCOafSo"
   },
   "outputs": [],
   "source": [
    "hvFlip = createTechnique(\"flip\",{\"flip\":-1})\n",
    "augmentor.addTransformer(transformer(hvFlip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ULr-hKyafSr"
   },
   "source": [
    "Showing the result of applying the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2Ko-lGkafSs",
    "outputId": "387f81be-6826-432a-af65-e1beed55457e"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "showBoxes(img,boxes)\n",
    "hvFlipGenerator = transformer(hvFlip)\n",
    "hvFlipImg,hvFlipBoxes = hvFlipGenerator.transform(img,boxes)\n",
    "plt.figure()\n",
    "plt.title(\"Transformed\")\n",
    "showBoxes(hvFlipImg,hvFlipBoxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZzS82tnafSw"
   },
   "source": [
    "#### Rotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJ7eV97aafSx"
   },
   "outputs": [],
   "source": [
    "rotate = createTechnique(\"rotate\", {\"angle\" : 90})\n",
    "augmentor.addTransformer(transformer(rotate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smI7ogocafSz"
   },
   "source": [
    "Showing the result of applying the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NH56U8N-afS0",
    "outputId": "29580393-c069-4230-8685-7de737857ed8"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "showBoxes(img,boxes)\n",
    "rotateGenerator = transformer(rotate)\n",
    "rotateImg,rotateBoxes = rotateGenerator.transform(img,boxes)\n",
    "plt.figure()\n",
    "plt.title(\"Transformed\")\n",
    "showBoxes(rotateImg,rotateBoxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXXVAlmNafS4"
   },
   "source": [
    "#### Average blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_SSywejafS4"
   },
   "outputs": [],
   "source": [
    "avgBlur =  createTechnique(\"average_blurring\", {\"kernel\" : 5})\n",
    "augmentor.addTransformer(transformer(avgBlur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYUtZGZYafS7"
   },
   "source": [
    "Showing the result of applying the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihZmSF5cafS8",
    "outputId": "ac56c851-3840-48fb-86e2-b0ee7df2b7e0"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "showBoxes(img,boxes)\n",
    "avgBlurGenerator = transformer(avgBlur)\n",
    "avgBlurImg,avgBlurBoxes = avgBlurGenerator.transform(img,boxes)\n",
    "plt.figure()\n",
    "plt.title(\"Transformed\")\n",
    "showBoxes(avgBlurImg,avgBlurBoxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXkbUjOWafTA"
   },
   "source": [
    "#### Raise Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2SVb0kyafTC"
   },
   "outputs": [],
   "source": [
    "hue = createTechnique(\"raise_hue\", {\"power\" : 0.9})\n",
    "augmentor.addTransformer(transformer(hue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTSEuBsVafTE"
   },
   "source": [
    "Showing the result of applying the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3kCoRtRafTF",
    "outputId": "6ee353b4-390a-46c8-8f53-e7cb9044a012"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "showBoxes(img,boxes)\n",
    "hueGenerator = transformer(hue)\n",
    "hueImg,hueBoxes = hueGenerator.transform(img,boxes)\n",
    "plt.figure()\n",
    "plt.title(\"Transformed\")\n",
    "showBoxes(hueImg,hueBoxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaNg2ruiafTJ"
   },
   "source": [
    "#### None\n",
    "(to keep also the original image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-m2oNi_AafTL"
   },
   "outputs": [],
   "source": [
    "none = createTechnique(\"none\",{})\n",
    "augmentor.addTransformer(transformer(none))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ouZ-I-DbafTO"
   },
   "source": [
    "## Applying the augmentation process\n",
    "\n",
    "Finally, we apply the augmentation process (this might take some time depending on the number of images of the original dataset and the number of transformations that will be applied). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owRG290NafTO",
    "outputId": "1ce0e2d1-67a2-4740-b3f1-0a8653f6174b"
   },
   "outputs": [],
   "source": [
    "augmentor.applyAugmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZydpyBjTafTT"
   },
   "source": [
    "Finally, we can check the amount of images in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atu1eEVUafTU",
    "outputId": "cfea033f-ca14-4800-889c-294691d30027"
   },
   "outputs": [],
   "source": [
    "print(\"Number of images in the folder\")\n",
    "!ls -1 augmented_images_yolo/*.jpg | wc -l\n",
    "print(\"Number of annotations in the folder\")\n",
    "!ls -1 augmented_images_yolo/*.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NnlkZUUrafTX"
   },
   "source": [
    "If you are executing this notebook in Colaboratory, you need to download the generated files. To that aim, you can create a zip folder and download it using the following commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r augmented_images_yolo.zip augmented_images_yolo\n",
    "from google.colab import files\n",
    "files.download('augmented_images_yolo.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CLODSA_Plants.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
