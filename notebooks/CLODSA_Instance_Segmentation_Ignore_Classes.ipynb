{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/joheras/CLoDSA/blob/master/notebooks/CLODSA_Instance_Segmentation_Ignore_Classes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxQekSqTht6I"
   },
   "source": [
    "# Augmenting a dataset for instance segmentation ignoring some classes\n",
    "\n",
    "In this notebook, we illustrate how CLODSA can be employed to augment a dataset of images devoted to instance segmentation that was annotated using the [COCO format](http://cocodataset.org/#home). In addition, we will ignore some of the classes of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YC_9o6-Jht6M"
   },
   "source": [
    "We will use a small dataset of shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3838
    },
    "colab_type": "code",
    "id": "QNhd1Ttiht6Q",
    "outputId": "0268f0b6-eecc-436a-ee85-05915b5f5dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-29 09:20:37--  https://www.dropbox.com/s/sjt0lo4p0dah54m/shapes-bis.zip?dl=0\n",
      "Resolviendo www.dropbox.com (www.dropbox.com)... 162.125.68.1, 2620:100:6024:1::a27d:4401\n",
      "Conectando con www.dropbox.com (www.dropbox.com)[162.125.68.1]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 301 Moved Permanently\n",
      "Ubicación: /s/raw/sjt0lo4p0dah54m/shapes-bis.zip [siguiente]\n",
      "--2019-08-29 09:20:37--  https://www.dropbox.com/s/raw/sjt0lo4p0dah54m/shapes-bis.zip\n",
      "Reutilizando la conexión con www.dropbox.com:443.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com/cd/0/inline/Ang9Ji2YZkd8WTFPxcKsWzZxu0yQg3EUsHYtZlvOWh03y67FgcnEXaMio9kvyoOYX101sjkrcpo6mfyEAmJssE0Wp1WtzaFNgjdbZUw6XE6Y8oTaBByyRaIALhLKAWjJKUM/file# [siguiente]\n",
      "--2019-08-29 09:20:38--  https://uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com/cd/0/inline/Ang9Ji2YZkd8WTFPxcKsWzZxu0yQg3EUsHYtZlvOWh03y67FgcnEXaMio9kvyoOYX101sjkrcpo6mfyEAmJssE0Wp1WtzaFNgjdbZUw6XE6Y8oTaBByyRaIALhLKAWjJKUM/file\n",
      "Resolviendo uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com (uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com)... 162.125.68.6, 2620:100:6024:6::a27d:4406\n",
      "Conectando con uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com (uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com)[162.125.68.6]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 FOUND\n",
      "Ubicación: /cd/0/inline2/AnhzfWs-Meb71cRdNcWjyBAO0plk2zYOaGeu_5UWhb9ih6jbOhXpog_kVhwb-TLy5RptHCWlVhgXi701nx8MBsE5adSyBXPa4oUNvzeZT2i0aUnqUG-LhTqupn7k9LryTLxiEn3H2JchyzLJXRIodzgOz_RJiaOPR_-A78Uem682b2SoPNr63bIbPghOqjIFklLhgHmfhIESf7Uyv6DGnC1w6DIw0CZT0yxD_2fsUt0tCaN8dSMhnl6nQ7JeHT4VacKMTYP6pncphmMaB3ovmdMkhrgXFXrflwogfK0VC43rMKJsg3izBo28u19BLEj3lPr1DXbeadhhuXhsMlDmxfgkSzIXolPQ7sI1NDg2CUxRVg/file [siguiente]\n",
      "--2019-08-29 09:20:39--  https://uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com/cd/0/inline2/AnhzfWs-Meb71cRdNcWjyBAO0plk2zYOaGeu_5UWhb9ih6jbOhXpog_kVhwb-TLy5RptHCWlVhgXi701nx8MBsE5adSyBXPa4oUNvzeZT2i0aUnqUG-LhTqupn7k9LryTLxiEn3H2JchyzLJXRIodzgOz_RJiaOPR_-A78Uem682b2SoPNr63bIbPghOqjIFklLhgHmfhIESf7Uyv6DGnC1w6DIw0CZT0yxD_2fsUt0tCaN8dSMhnl6nQ7JeHT4VacKMTYP6pncphmMaB3ovmdMkhrgXFXrflwogfK0VC43rMKJsg3izBo28u19BLEj3lPr1DXbeadhhuXhsMlDmxfgkSzIXolPQ7sI1NDg2CUxRVg/file\n",
      "Reutilizando la conexión con uc0dfb5a815d611ff0daedc8feea.dl.dropboxusercontent.com:443.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 10525 (10K) [application/zip]\n",
      "Guardando como: “shapes-bis.zip”\n",
      "\n",
      "shapes-bis.zip      100%[===================>]  10,28K  --.-KB/s    en 0s      \n",
      "\n",
      "2019-08-29 09:20:39 (196 MB/s) - “shapes-bis.zip” guardado [10525/10525]\n",
      "\n",
      "Archive:  shapes-bis.zip\n",
      "   creating: shapes-bis/\n",
      "  inflating: shapes-bis/1000.jpeg    \n",
      "  inflating: shapes-bis/1116.jpeg    \n",
      "  inflating: shapes-bis/1046.jpeg    \n",
      "  inflating: shapes-bis/1140.jpeg    \n",
      "  inflating: shapes-bis/1182.jpeg    \n",
      "  inflating: shapes-bis/annotations.json  \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/sjt0lo4p0dah54m/shapes-bis.zip?dl=0 -O shapes-bis.zip\n",
    "!unzip shapes-bis.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAy0Hvnbht6i"
   },
   "source": [
    "We can check the elements of the shapes folder that are a json file with the coco annotation and two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "XEJ0pRfRht6k",
    "outputId": "9cc7906d-f241-403d-b41e-7b8b4ff9112c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.jpeg  1046.jpeg  1116.jpeg  1140.jpeg  1182.jpeg  annotations.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls shapes-bis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaBilQHUht6u"
   },
   "source": [
    "## Augmentation techniques\n",
    "\n",
    "For this example, we consider three augmentation techniques. \n",
    "\n",
    "The augmentation techniques applied in this example are:\n",
    "- Rotation.\n",
    "- Flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCILufF2ht6y"
   },
   "source": [
    "## Installing the necessary libraries\n",
    "\n",
    "In case that CLODSA is not installed in your system, the first task consists in installing it using ``pip``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "id": "FWTm8dG3ht6y",
    "outputId": "ae3a8866-cbfb-4863-a357-248e1e56989c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clodsa==1.2.32\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/4d/1de07129522b4a3d60e4b366339570b4cfd0736715fba2333c737f4584fd/clodsa-1.2.32.tar.gz\n",
      "Requirement already satisfied: mahotas in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (1.4.5)\n",
      "Requirement already satisfied: imutils in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (0.5.1)\n",
      "Requirement already satisfied: Keras in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (2.1.6)\n",
      "Requirement already satisfied: commentjson in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (0.7.1)\n",
      "Requirement already satisfied: scipy in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (1.1.0)\n",
      "Requirement already satisfied: h5py in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (2.7.1)\n",
      "Requirement already satisfied: numpy in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (1.12.1)\n",
      "Requirement already satisfied: progressbar2 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (3.38.0)\n",
      "Requirement already satisfied: scikit_learn in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from clodsa==1.2.32) (0.19.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from Keras->clodsa==1.2.32) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from Keras->clodsa==1.2.32) (3.13)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (from progressbar2->clodsa==1.2.32) (2.3.0)\n",
      "Building wheels for collected packages: clodsa\n",
      "  Running setup.py bdist_wheel for clodsa ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jonathan/.cache/pip/wheels/8e/43/d7/77b5712fb239c468ccb0f8c9af9e6d1becda97134a31e39a9f\n",
      "Successfully built clodsa\n",
      "Installing collected packages: clodsa\n",
      "  Found existing installation: clodsa 1.2.28\n",
      "    Uninstalling clodsa-1.2.28:\n",
      "      Successfully uninstalled clodsa-1.2.28\n",
      "Successfully installed clodsa-1.2.32\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install clodsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1q3x_OFht66"
   },
   "source": [
    "## Loading the necessary libraries\n",
    "\n",
    "The first step in the pipeline consists in loading the necessary libraries to apply the data augmentation techniques in CLODSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JqWBswFyht68",
    "outputId": "77349188-126d-4e4d-93fa-929ec33ad573"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from clodsa.augmentors.augmentorFactory import createAugmentor\n",
    "from clodsa.transformers.transformerFactory import transformerGenerator\n",
    "from clodsa.techniques.techniqueFactory import createTechnique\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBP59dqqht7E"
   },
   "source": [
    "## Creating the augmentor object\n",
    "\n",
    "As explained in the documentation of CLODSA, we need to specify some parameters for the augmentation process, and use them to create an augmentor object.  \n",
    "\n",
    "_The kind of problem_. In this case, we are working in an instance segmentation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQ5q8WVnht7G"
   },
   "outputs": [],
   "source": [
    "PROBLEM = \"instance_segmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D46gdf-4ht7K"
   },
   "source": [
    "_The annotation mode_. The annotation is provided using the coco format in a file called annotations.json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrlRg-FVht7M"
   },
   "outputs": [],
   "source": [
    "ANNOTATION_MODE = \"coco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn-uF33Oht7S"
   },
   "source": [
    "_The input path_. The input path containing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78jPXCj2ht7U"
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = \"shapes-bis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9kGGhs4ht7a"
   },
   "source": [
    "_The generation mode_. In this case, linear, that is, all the augmentation techniques are applied to all the images of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCweCzLeht7c"
   },
   "outputs": [],
   "source": [
    "GENERATION_MODE = \"linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6WljljVht7g"
   },
   "source": [
    "_The output mode_. The generated images will be stored in a new folder called output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4uKKcJUht7i"
   },
   "outputs": [],
   "source": [
    "OUTPUT_MODE = \"coco\"\n",
    "OUTPUT_PATH= \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, there are three classes with ids 1, 2, and 3. We are going to ignore the class 3. To that aim, we create the following set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_CLASSES = {3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9R79LEvVht7o"
   },
   "source": [
    "Using the above information, we can create our augmentor object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQ9wyiQuht7q"
   },
   "outputs": [],
   "source": [
    "augmentor = createAugmentor(PROBLEM,ANNOTATION_MODE,OUTPUT_MODE,GENERATION_MODE,INPUT_PATH,\n",
    "                            {\"outputPath\":OUTPUT_PATH,\"ignoreClasses\":IGNORE_CLASSES})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXOfuq90ht7w"
   },
   "source": [
    "## Adding the augmentation techniques\n",
    "\n",
    "Now, we define the techniques that will be applied in our augmentation process and add them to our augmentor object. To illustrate the transformations, we will use the following image of the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we must define a transformer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformerGenerator(PROBLEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4PfSKW-ht74"
   },
   "source": [
    "_Rotations:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajKE-mkDht74"
   },
   "outputs": [],
   "source": [
    "for angle in [90,180]:\n",
    "    rotate = createTechnique(\"rotate\", {\"angle\" : angle})\n",
    "    augmentor.addTransformer(transformer(rotate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Flips:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = createTechnique(\"flip\",{\"flip\":1})\n",
    "augmentor.addTransformer(transformer(flip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwE-qSYLht9I"
   },
   "source": [
    "## Applying the augmentation process\n",
    "\n",
    "Finally, we apply the augmentation process (this might take some time depending on the number of images of the original dataset and the number of transformations that will be applied). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbW5YVE9ht9I"
   },
   "outputs": [],
   "source": [
    "augmentor.applyAugmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the elements of the output folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_1000.jpeg  0_1140.jpeg  1_1116.jpeg  2_1000.jpeg  2_1182.jpeg\r\n",
      "0_1046.jpeg  0_1182.jpeg  1_1140.jpeg  2_1116.jpeg  annotation.json\r\n",
      "0_1116.jpeg  1_1000.jpeg  1_1182.jpeg  2_1140.jpeg\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the results using some of the tools provided by [the COCO API](https://github.com/cocodataset/cocoapi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /home/jonathan/.virtualenvs/cv/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'output/'\n",
    "annotation_file = 'output/annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "example_coco = COCO(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom COCO categories: \n",
      "square circle triangle\n",
      "\n",
      "Custom COCO supercategories: \n",
      "shape\n"
     ]
    }
   ],
   "source": [
    "categories = example_coco.loadCats(example_coco.getCatIds())\n",
    "category_names = [category['name'] for category in categories]\n",
    "print('Custom COCO categories: \\n{}\\n'.format(' '.join(category_names)))\n",
    "\n",
    "category_names = set([category['supercategory'] for category in categories])\n",
    "print('Custom COCO supercategories: \\n{}'.format(' '.join(category_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each execution of the following cells show a different image of the output dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = example_coco.getCatIds(catNms=['square'])\n",
    "image_ids = example_coco.getImgIds(catIds=category_ids)\n",
    "image_data = example_coco.loadImgs(image_ids[np.random.randint(0, len(image_ids))])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3,\n",
       " 'file_name': '2_1000.jpeg',\n",
       " 'width': 128,\n",
       " 'height': 128,\n",
       " 'date_captured': '',\n",
       " 'license': 1,\n",
       " 'coco_url': '',\n",
       " 'flickr_url': ''}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = io.imread(image_directory + image_data['file_name'])\n",
    "plt.imshow(image); plt.axis('off')\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "annotation_ids = example_coco.getAnnIds(imgIds=image_data['id'], catIds=category_ids, iscrowd=None)\n",
    "annotations = example_coco.loadAnns(annotation_ids)\n",
    "example_coco.showAnns(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are executing this notebook in Colaboratory, you need to download the generated files. To that aim, you can create a zip folder and download it using the following commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r shapes.zip output\n",
    "from google.colab import files\n",
    "files.download('shapes.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CLODSA_Nuclei.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
